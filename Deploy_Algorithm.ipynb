{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the session\n",
    "\n",
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker as sage\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "smmp = boto3.client(\"sagemaker\")\n",
    "\n",
    "# TODO: Update algorithm_arn from your end\n",
    "algorithm_arn = \"arn:aws:sagemaker:us-west-2:512418328296:algorithm/neopoly-molecule-1727104855\"\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "common_prefix = \"neopoly-molecule-input\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Train your Algorithm\n",
    "\n",
    "A number of files are laid out for your use, under the `/opt/ml` directory:\n",
    "\n",
    "    /opt/ml\n",
    "    |-- input\n",
    "    |   |-- config\n",
    "    |   |   |-- hyperparameters.json\n",
    "    |   `-- data\n",
    "    |       `-- <channel_name>\n",
    "    |           `-- <input data>\n",
    "    |-- model\n",
    "    |   `-- <model files>\n",
    "    `-- output\n",
    "        `-- failure\n",
    "\n",
    "##### The input\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how your program runs. `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. \n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it's generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. \n",
    "\n",
    "##### The output\n",
    "\n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is a directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file will be returned in the `FailureReason` field of the `DescribeTrainingJob` result. For jobs that succeed, there is no reason to write this file as it will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3; prefix is the S3 bucket path and workdir is the local path\n",
    "training_input_prefix = common_prefix + \"/training-input-data\"\n",
    "TRAINING_WORKDIR = \"data/training\"\n",
    "training_input = sess.upload_data(\n",
    "    TRAINING_WORKDIR, key_prefix=training_input_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an algorithm etimator from the algorithm product ARN\n",
    "neopoly = sage.AlgorithmEstimator(\n",
    "    algorithm_arn=algorithm_arn,\n",
    "    base_job_name='neopoly-molecule-training',\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.r5.8xlarge',\n",
    "    output_path=\"s3://{}/neopoly-molecule-model\".format(sess.default_bucket()),\n",
    "    hyperparameters={ \n",
    "        \"epochs\" : \"10\",\n",
    "        \"t_0\": \"10\",\n",
    "        \"batch_size\": \"16\",\n",
    "        \"finetune_batch_size\": \"4\",\n",
    "        \"structure_lr\": \"0.01\",\n",
    "        \"property_lr\": \"0.005\",\n",
    "        \"alpha\": \"0.1\",\n",
    "        \"edge_threshold\": \"1.0\",\n",
    "        \"crps_threshold\": \"1.0\",\n",
    "        \"p_threshold\": \"0.0\",\n",
    "    }\n",
    ")\n",
    "\n",
    "neopoly.fit({'training': training_input})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Batch Transform Inference\n",
    "\n",
    "The training job produces an S3 model artifact with the trained weights. These can be loaded to create a model package, which we can use to run batch transformations with. In contrast to real-time inferences on an endpoint (Part 3), batch transformations allow larger datasets and is only billed per call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3; prefix is the S3 bucket path and workdir is the local path\n",
    "batch_inference_input_prefix = common_prefix + \"/batch-inference-input-data\"\n",
    "TRANSFORM_WORKDIR = \"data/transform\"\n",
    "transform_input = (\n",
    "    sess.upload_data(TRANSFORM_WORKDIR, key_prefix=batch_inference_input_prefix)\n",
    "    + \"/transform_test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve S3 model artifact from training job\n",
    "model_path = \"s3://sagemaker-us-west-2-512418328296/neopoly-molecule-model/neopoly-molecule-training-2024-09-27-15-16-51-494/output/model.tar.gz\"\n",
    "\n",
    "# Instantiate a model package from the model artifact\n",
    "model = sage.ModelPackage(\n",
    "    role=role, \n",
    "    model_data=model_path, \n",
    "    sagemaker_session=sess, \n",
    "    algorithm_arn=algorithm_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a transformer from the model\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.r7i.8xlarge'\n",
    ")\n",
    "\n",
    "# Run the batch transformation job\n",
    "transformer.transform(\n",
    "    transform_input, \n",
    "    job_name=\"neopoly-molecule-transform\",\n",
    "    content_type=\"text/csv\"\n",
    ")\n",
    "transformer.wait()\n",
    "\n",
    "# Output is available in the following path\n",
    "transformer.output_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Real-time Inference with a Persistent Endpoint\n",
    "\n",
    "Deploying the model to Amazon SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint. Prediction is as easy as calling predict with the predictor we got back from deploy and the data we want to do predictions with. The serializers take care of doing the data conversions for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running your container during hosting\n",
    "\n",
    "Hosting has a very different model than training because hosting is reponding to inference requests that come in via HTTP. In this example, we use our recommended Python serving stack to provide robust and scalable serving of inference requests.\n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "* `/ping` will receive `GET` requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these will be passed in as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "test_data_df = pd.read_csv(\"data/transform\" + \"/transform_test.csv\")\n",
    "test_data_csv = test_data_df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy to persistent endpoint\n",
    "predictor = neopoly.deploy(1, \"ml.r7i.8xlarge\")\n",
    "\n",
    "# Endpoint input configurations\n",
    "client = boto3.client('sagemaker-runtime', region_name='us-west-2')\n",
    "try:\n",
    "    endpoint_name = predictor.endpoint\n",
    "except:\n",
    "    endpoint_name = \"neopoly-molecule-2024-09-27-04-14-08-066\"                                       \n",
    "\n",
    "# Invoke the endpoint\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    ContentType=\"text/csv\" ,\n",
    "    Body=test_data_csv\n",
    "    )\n",
    "\n",
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
